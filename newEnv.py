# -*- coding: utf-8 -*-
"""AI Project 2025/2026

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1UqUvt21_kjK6ORJJ_zLPsipj9QC8rx5O

# RL basics : learning how to sail

## Introduzione

In questo notebook creeremo un ambiente di simulazione per una barca a vela usando OpenAI Gym, e alleneremo un agente con Deep Q-Learning (DQN) per raggiungere un waypoint target.

### Componenti principali:
1. **Stato**: posizione (x,y), velocitÃ , heading, vento (direzione e intensitÃ )
2. **Azioni**: cambiare rotta (sinistra, destra, dritto) e regolare le vele
3. **Reward**: basato sulla distanza dal target e velocitÃ 
4. **Fisica semplificata**: polar diagram e VMG (Velocity Made Good) super semplificati

### 1. Importiamo le librerie
"""

!pip install gymnasium stable-baselines3 torch numpy matplotlib opencv-python imageio pettingzoo supersuit

import numpy as np
import gymnasium as gym
from gymnasium import spaces
import matplotlib.pyplot as plt
import matplotlib.patches as patches
from matplotlib.animation import FuncAnimation
from IPython.display import HTML
import torch
from stable_baselines3 import DQN, PPO
from stable_baselines3.common.callbacks import BaseCallback
import imageio
from typing import Optional
import os
from stable_baselines3.common.monitor import Monitor

"""Then you can try to plot some graphs on how the model perfomed across some tests and see quantitatively how the training wwnt and how you can improve.

# Molteplici Barche

### Codice multi agente

- Ambiente : PettingZoo (ParallelEnv)
- Training : StableBaselines3 + Supersuit
"""

import functools
import gymnasium as gym
from gymnasium import spaces
import numpy as np
import matplotlib.pyplot as plt
import matplotlib.patches as patches
from pettingzoo import ParallelEnv

class MultiAgentSailingZoo(ParallelEnv):
    metadata = {"render_modes": ["rgb_array", "human"], "name": "sailing_v1"}

    def __init__(self, field_size=400, render_mode=None):
        super().__init__()

        self.field_size = field_size
        self.render_mode = render_mode

        # Physical Parameters
        self.max_speed = 15.0
        self.target_radius = 20.0
        self.boat_radius = 5.0
        self.dt = 1.0
        self.max_steps = 250

        # Agents
        self.possible_agents = ["boat_0", "boat_1"]
        self.agents = self.possible_agents[:]

        # RL spaces
        self._obs_space = spaces.Box(low=-1.0, high=1.0, shape=(14,), dtype=np.float32)
        self._act_space = spaces.Discrete(3) # 0: SX, 1: Dritto, 2: DX
        self.observation_spaces = {agent: self._obs_space for agent in self.possible_agents}
        self.action_spaces = {agent: self._act_space for agent in self.possible_agents}

        # States variables
        self.boat_states = {}
        self.target = []
        self.wind_direction = None
        self.wind_speed = None
        self.step_count = 0
        self.trajectories = {a: [] for a in self.agents}
        self.previous_distances = {a: 0.0 for a in self.agents}
        self.best_distances = {a: 0.0 for a in self.agents}
        self.global_best_distance = {a: np.inf for a in self.agents}
        self.winner = None

        # Stats variables
        self.stat_cumulative_vmg = {}
        self.stat_cumulative_polar = {}
        self.stat_total_dist = {}
        self.stat_initial_dist = {}

        self.np_random = None
        self.fig = None
        self.ax = None

    def observation_space(self, agent):
        return self.observation_spaces[agent]

    def action_space(self, agent):
        return self.action_spaces[agent]

    def reset(self, seed=None, options=None):
        self.agents = self.possible_agents[:]
        # Reset global variables used in step
        self.step_count = 0
        self.winner = None

        if seed is not None:
            self.np_random = np.random.RandomState(seed)
        elif self.np_random is None:
            self.np_random = np.random.RandomState()

        # Target Position
        self.target = np.array([
            self.np_random.uniform(self.field_size - 200, self.field_size - 200),
            self.np_random.uniform(self.field_size - 50, self.field_size - 50)
        ])

        # Wind
        self.wind_direction = np.pi/2
        self.wind_speed = self.np_random.uniform(10, 18)
        self.wind_change_steps = 25

        # Boat reset
        start_x = self.np_random.uniform(100, 300)
        start_y = self.np_random.uniform(50, 50)
        self.boat_states = {}
        self.trajectories = {a: [] for a in self.agents}
        self.previous_distances = {}
        heading = self.np_random.uniform(0, 2*np.pi)

        # Reset stats variables
        self.last_action = {a: None for a in self.agents}
        self.turn_streak = {a: 0 for a in self.agents}
        self.stat_triple_turns = {a: 0 for a in self.agents}

        # Random offset to create fairness in training
        spawn_offsets = [0, 20]
        self.np_random.shuffle(spawn_offsets)
        for i, agent in enumerate(self.agents):
            offset = spawn_offsets[i]
            self.boat_states[agent] = {
                'x': start_x + offset,
                'y': start_y,
                'speed': 0.0,
                'heading': heading,
                'finished': False,
                'max_speed_hit': 0.0,
                'is_inside': False,
                'is_inside_curr': False,
                'is_near_target': False
            }
            self.trajectories[agent].append(np.array([self.boat_states[agent]['x'], self.boat_states[agent]['y']]))

            # Initial distance to target
            pos = np.array([self.boat_states[agent]['x'], self.boat_states[agent]['y']])
            dist = np.linalg.norm(pos - self.target)

            self.stat_initial_dist[agent] = dist
            self.best_distances[agent] = dist
            # Reset stats variables
            self.stat_total_dist[agent] = 0.0
            self.stat_cumulative_vmg[agent] = 0.0

        target_x = self.target[0]

        # Compute distance of boats to the axis x of the target
        x_distances = {}
        for agent in self.agents:
            boat_x = self.boat_states[agent]['x']
            dist_x = abs(target_x - boat_x)
            x_distances[agent] = dist_x
            print(boat_x)

        # Find minimum distance between boats
        min_dist_x = min(x_distances.values())

        # Assign 'is_inside' to the boats that are closest (for stats)
        for agent in self.agents:
            if x_distances[agent] == min_dist_x:
                self.boat_states[agent]['is_inside'] = True
            else:
                self.boat_states[agent]['is_inside'] = False

        observations = {a: self._get_single_obs(a) for a in self.agents}
        infos = {a: {} for a in self.agents}

        return observations, infos

    def step(self, actions):
        if not self.agents:
            return {}, {}, {}, {}, {}

        # Instantiate dictionaries
        rewards = {a: 0.0 for a in self.possible_agents}
        terminations = {a: False for a in self.possible_agents}
        truncations = {a: False for a in self.possible_agents}
        infos = {a: {} for a in self.possible_agents}

        self.step_count += 1


        # Create random order of actions to create fairness in training
        agents_order = self.agents[:]
        self.np_random.shuffle(agents_order)

        # Physics and action appliation
        for agent in agents_order:
            if agent not in actions: continue # If the agent is dead/finished, skips

            action = actions[agent]
            state = self.boat_states[agent]

            if state['finished']: continue

            # Check if the agent chooses the same action (apart from the straight one) three times in a row
            if action == 1:
                self.turn_streak[agent] = 0
            else:
                if action == self.last_action[agent]:
                    self.turn_streak[agent] += 1
                else:
                    self.turn_streak[agent] = 1

                if self.turn_streak[agent] == 3:
                    self.stat_triple_turns[agent] += 1
                    self.turn_streak[agent] = 0

            self.last_action[agent] = action

            # Apply rotation or keep sailing forward
            if action == 0:
              state['heading'] -= np.radians(15)
            elif action == 2:
              state['heading'] += np.radians(15)
            state['heading'] = state['heading'] % (2 * np.pi)

            # Velocity with polar diagram
            apparent_wind = self.wind_direction - state['heading']
            current_wind_speed = self.wind_speed

            # Identify opponents
            opponents = [opponent for opponent in self.possible_agents if opponent != agent and not self.boat_states[opponent]['finished']]

            # Compute shadow box
            for opp in opponents:
                opp_state = self.boat_states[opp]

                # Distance between boats
                dx = state['x'] - opp_state['x']
                dy = state['y'] - opp_state['y']
                dist = np.hypot(dx,dy)

                # Angle of wind in relation to the vector of the boats
                wind_vec = -np.array([np.cos(self.wind_direction), np.sin(self.wind_direction)])
                pos_vec = np.array([dx, dy])

                # Projection: positive (im in shadow), negative (not in shadow)
                proj = np.dot(wind_vec, pos_vec)

                # Check if boat is in shadow and close to being affected by it
                if proj > 0 and dist < (self.boat_radius * 10):
                    cos_angle = np.clip(proj / (dist + 1e-6), -1.0, 1.0)
                    angle_diff = np.arccos(cos_angle)
                    if angle_diff < np.radians(20):
                        # Decrease speed while in shadow box
                        current_wind_speed = current_wind_speed * 0.6

            # Real speed
            state['speed'] = self._get_polar_speed(apparent_wind, current_wind_speed)

            if state['speed'] > state['max_speed_hit']:
                state['max_speed_hit'] = state['speed']

            # Boat Movement
            displacement = state['speed'] * 0.514 * self.dt
            state['x'] += displacement * np.cos(state['heading'])
            state['y'] += displacement * np.sin(state['heading'])

            self.trajectories[agent].append(np.array([state['x'], state['y']]))


            self.stat_total_dist[agent] += displacement

        # Collision computing (only for 2 boats to change if using more boats)
        collision = False
        attention_radius = self.boat_radius * 4
        agent_lists = [a for a in self.possible_agents]
        if len(agent_lists) >= 2:
            boat_0 = agent_lists[0]
            boat_1 = agent_lists[1]
            pos_0 = np.array([self.boat_states[boat_0]['x'], self.boat_states[boat_0]['y']])
            pos_1 = np.array([self.boat_states[boat_1]['x'], self.boat_states[boat_1]['y']])

            distance_between_boats = np.linalg.norm (pos_1 - pos_0)
            if distance_between_boats < (self.boat_radius * 2):
                collision = True

            elif distance_between_boats < attention_radius:
                    proximity_penalty = (attention_radius - distance_between_boats) / attention_radius * 5.0
                    rewards[boat_0] -= proximity_penalty
                    rewards[boat_1] -= proximity_penalty

        # Stochastic wind
        if (self.step_count % self.wind_change_steps) == 0:
              self.wind_change_range = self.np_random.uniform(-np.radians(10),np.radians(10))
              self.wind_direction = np.clip(self.wind_direction + self.wind_change_range, 0, np.pi)


        # Check who is inside currently for who can overtake
        x_distances = {}
        for agent in self.agents:
            boat_x = self.boat_states[agent]['x']
            dist_x = abs(self.target[0] - boat_x)
            x_distances[agent] = dist_x


        min_dist_x = min(x_distances.values())

        for agent in self.agents:
            if x_distances[agent] == min_dist_x:
                self.boat_states[agent]['is_inside_curr'] = True
            else:
                self.boat_states[agent]['is_inside_curr'] = False

        # Reward calculation
        for agent in agents_order:
            state = self.boat_states[agent]
            pos = np.array([state['x'], state['y']])
            dist_to_target = np.linalg.norm(pos - self.target)

            state['is_near_target'] = dist_to_target < self.target_radius * 3

            if state['finished']:
                terminations[agent] = True
                continue

            # VMG Reward
            to_target_angle = np.arctan2(self.target[1] - state['y'], self.target[0] - state['x'])
            angle_error = to_target_angle - state['heading']
            vmg = state['speed'] * np.cos(angle_error)
            self.stat_cumulative_vmg[agent] += vmg
            rewards[agent] = rewards[agent] + (vmg * 0.7)

            # Time penalty
            rewards[agent] -= 0.05

            # Deadzone Penalty
            apparent_wind = self.wind_direction - state['heading']
            wind_angle_rel = np.abs(np.degrees(apparent_wind)) % 360
            if wind_angle_rel > 180: wind_angle_rel = 360 - wind_angle_rel
            if wind_angle_rel < 20:
                rewards[agent] -= 0.5

            # Overtaking
            displacement = state['speed'] * 0.514 * self.dt
            dx_me = np.cos(state['heading']) * displacement
            dy_me = np.sin(state['heading']) * displacement

            for opp in opponents:
                  displacement_opp = self.boat_states[opp]['speed'] * 0.514 * self.dt
                  dx_opp = np.cos(self.boat_states[opp]['heading']) * displacement_opp
                  dy_opp = np.sin(self.boat_states[opp]['heading']) * displacement_opp
                  det = dx_me * dy_opp - dx_opp * dy_me


                  x_opp = self.boat_states[opp]['x']
                  y_opp = self.boat_states[opp]['y']


                  dx_pos = x_opp - state['x']
                  dy_pos = y_opp - state['y']

                  opp_pos = np.array([x_opp,y_opp])
                  opp_dist_to_target = np.linalg.norm(opp_pos - self.target)

                  if det != 0:

                    # Number of steps where they intersecate in the projection
                    t = (dx_pos * dy_opp - dy_pos * dx_opp ) / det
                    u = (dx_pos * dy_me - dy_pos * dx_me ) / det

                    if 0 <= t <= 10 and 0 <= u <= 10:
                      # Penalize boats that are not inside currently if they try to overtake
                      if not state['is_inside_curr'] and not state['is_near_target']:
                          rewards[agent] -= 3
                      else:
                          rewards[agent] -= 0.05


            # Reward to boats that didn't win if the beat their personal best distance (closer to target)
            if state['finished'] and agent != self.winner and dist_to_target < self.global_best_distance[agent]:
              rewards[agent] += (self.global_best_distance[agent] - dist_to_target) * 0.5
              self.global_best_distance[agent] = dist_to_target

            self.best_distances[agent] = min(self.best_distances[agent], dist_to_target)

            # Penalty for collision
            if collision:
                rewards[agent] -= 200.0
                state['finished'] = True
                terminations[agent] = True

            # Target Reached
            if dist_to_target < self.target_radius:
              if self.winner == None:
                # Reward for the first boat
                rewards[agent] += 100.0
                state['finished'] = True
                terminations[agent] = True
                self.winner = agent
              elif agent != self.winner:
                # Reward for the other boats that reach the target (to learn path and dont surrend during training)
                rewards[agent] += 50.0
                state['finished'] = True
                terminations[agent] = True

                # Scaled penalty for the boats that dont reach first (to keep competitiveness)
                for opp in self.agents:
                    if opp != agent:
                        opp_pos = np.array([self.boat_states[opp]['x'], self.boat_states[opp]['y']])
                        opp_dist_to_target = np.linalg.norm(opp_pos - self.target)

                        max_penalty = 30.0
                        min_penalty = 5.0
                        distance_ratio = np.clip(opp_dist_to_target /(self.target_radius * 10), 0.0, 1.0)
                        scaled_penalty = min_penalty + (max_penalty - min_penalty) * distance_ratio
                        rewards[opp] -= scaled_penalty

            # Out of bounds
            if not (0 <= state['x'] <= self.field_size and 0 <= state['y'] <= self.field_size):
                rewards[agent] -= 200.0
                state['finished'] = True
                terminations[agent] = True

            # Max steps
            if self.step_count >= self.max_steps:
                truncations[agent] = True


        # Statistic calculations
        for a in self.possible_agents:
            # Base info
            infos[a]['is_winner'] = (self.winner == a)
            infos[a]['max_speed'] = float(self.boat_states[a]['max_speed_hit']) if a in self.boat_states else 0.0

            steps = max(1, self.step_count)

            # 1. Avg VMG
            # Note: stat_cumulative_vmg is accumulated at every step
            infos[a]['avg_vmg'] = float(self.stat_cumulative_vmg[a] / steps)

        # Console output at race end
        # Filter active agents for rendering or internal logic
        self.agents = [a for a in self.agents if not (terminations[a] or truncations[a])]

        observations = {a: self._get_single_obs(a) for a in self.possible_agents}

        if not self.agents: # If everyone finished
            reason = "Target reached" if self.winner else ("Collision" if collision else "Timeout/Out of Bounds")
            print(f"\n{'='*40}")
            print(f"ðŸ RACE END - Step: {self.step_count} | Cause: {reason}")
            for a in self.possible_agents:
                status = "ðŸ†" if self.winner == a else "âŒ"
                vmg = infos[a]['avg_vmg']
                dist_tot = self.stat_total_dist[a]
                triples = self.stat_triple_turns[a]
                print(f" {a}: {status} | VMG Avg: {vmg:.2f} | Dist: {dist_tot:.1f}m | Triple Turns: {triples}")
            print(f"{'='*40}\n")

        return observations, rewards, terminations, truncations, infos

    def _get_polar_speed(self, apparent_wind_angle, wind_speed):
        angle_deg = np.abs(np.degrees(apparent_wind_angle) % 360)
        if angle_deg > 180:
            angle_deg = 360 - angle_deg

        if angle_deg < 20:
            speed_ratio = 0.0
        elif angle_deg < 50:
            speed_ratio = 0.2 + (angle_deg - 20) * 0.02
        elif angle_deg < 90:
            speed_ratio = 0.4 + (angle_deg - 50) * 0.0075
        elif angle_deg < 120:
            speed_ratio = 0.7
        elif angle_deg < 150:
            speed_ratio = 0.7 - (angle_deg - 120) * 0.003
        else:
            speed_ratio = 0.6 - (angle_deg - 150) * 0.005

        return min(speed_ratio * wind_speed, self.max_speed)

    def _get_single_obs(self, agent_id):
        if agent_id not in self.boat_states:
             return np.zeros(14, dtype=np.float32)

        me = self.boat_states[agent_id]
        pos = np.array([me['x'], me['y']])
        dist_to_target = np.linalg.norm(pos - self.target)
        angle_to_target = np.arctan2(self.target[1] - pos[1], self.target[0] - pos[0])
        apparent_wind = self.wind_direction - me['heading']

        opp_id = "boat_1" if agent_id == "boat_0" else "boat_0"
        opp = self.boat_states[opp_id]
        opp_pos = np.array([opp['x'], opp['y']])
        rel_pos = opp_pos - pos

        obs = np.array([
            me['x'] / self.field_size,
            me['y'] / self.field_size,
            me['speed'] / self.max_speed,
            np.cos(me['heading']),
            np.sin(me['heading']),
            np.cos(apparent_wind),
            np.sin(apparent_wind),
            np.cos(angle_to_target),
            np.sin(angle_to_target),
            dist_to_target / (self.field_size * np.sqrt(2)),
            rel_pos[0] / self.field_size,
            rel_pos[1] / self.field_size,
            np.cos(opp['heading']),
            np.sin(opp['heading'])
        ], dtype=np.float32)
        return obs

    def _normalize_angle(self, angle):
        return (angle + np.pi) % (2 * np.pi) - np.pi

    def render(self):
        if self.render_mode == 'rgb_array' or self.render_mode == 'human':
            return self._render_frame()

    def _render_frame(self):
        if self.fig is not None:
            plt.close(self.fig)

        self.fig, self.ax = plt.subplots(figsize=(8, 8))
        self.ax.set_xlim(0, self.field_size)
        self.ax.set_ylim(0, self.field_size)
        self.ax.set_aspect('equal')
        self.ax.grid(True, alpha=0.3)

        # Wind drawing
        wind_arrow_center_x = 40
        wind_arrow_center_y = self.field_size - 40
        arrow_length = 30
        dx = arrow_length * np.cos(self.wind_direction)
        dy = arrow_length * np.sin(self.wind_direction)
        wind_degrees = int(np.degrees(self.wind_direction) % 360)

        self.ax.arrow(wind_arrow_center_x, wind_arrow_center_y, -dx, -dy,
                      head_width=10, head_length=10, fc='cyan', ec='blue', alpha=0.8, width=2)
        self.ax.text(wind_arrow_center_x, wind_arrow_center_y + 25,
                     f"Wind: {self.wind_speed:.1f} ({wind_degrees}Â°)", color='blue',
                     ha='center', fontsize=9, weight='bold')

        # Target
        target_circle = patches.Circle(self.target, self.target_radius,
                                       color='red', alpha=0.3, label='Target')
        self.ax.add_patch(target_circle)


        # Shadow cones to see where they "hit"
        for agent_id, state in self.boat_states.items():
            if state['finished']: continue

            # Calculate cone vertex (boat center)
            start_point = np.array([state['x'], state['y']])

            # Calculate wind direction (WHERE wind goes)
            wind_vec_x = -np.cos(self.wind_direction)
            wind_vec_y = -np.sin(self.wind_direction)

            shadow_length = self.boat_radius * 10  # Shadow length
            cone_width_angle = np.radians(20)      # Angle used in step() logic

            # Calculate the two sides of the shadow triangle
            angle_wind = np.arctan2(wind_vec_y, wind_vec_x)

            x1 = state['x'] + shadow_length * np.cos(angle_wind - cone_width_angle)
            y1 = state['y'] + shadow_length * np.sin(angle_wind - cone_width_angle)

            x2 = state['x'] + shadow_length * np.cos(angle_wind + cone_width_angle)
            y2 = state['y'] + shadow_length * np.sin(angle_wind + cone_width_angle)

            # Draw polygon
            shadow_poly = patches.Polygon([start_point, [x1, y1], [x2, y2]],
                                          closed=True, color='gray', alpha=0.2)
            self.ax.add_patch(shadow_poly)

        future_steps = 30  # How many steps ahead to visualize

        for agent in self.agents:
            if self.boat_states[agent]['finished']: continue

            s = self.boat_states[agent]

            # Calculate displacement vector for single step (consistent with step())
            step_dist = s['speed'] * 0.514 * self.dt
            dx = np.cos(s['heading']) * step_dist
            dy = np.sin(s['heading']) * step_dist

            # Calculate projection end point
            end_x = s['x'] + dx * future_steps
            end_y = s['y'] + dy * future_steps

            # 1. dashed line (The Projection)
            self.ax.plot([s['x'], end_x], [s['y'], end_y],
                         linestyle='--', color='gray', alpha=0.5, linewidth=1)

        # --- 3. Agents Rendering ---
        colors = ['green', 'orange', 'purple', 'blue'] # Colors for different agents


        for i, (agent_id, state) in enumerate(self.boat_states.items()):
            color = colors[i % len(colors)]

            # Trajectory
            if agent_id in self.trajectories and len(self.trajectories[agent_id]) > 1:
                traj = np.array(self.trajectories[agent_id])
                self.ax.plot(traj[:, 0], traj[:, 1], color=color, linestyle='-', alpha=0.5, linewidth=1)

            # Boat Polygon
            boat_size = 15
            boat_points = np.array([
                [boat_size, 0],
                [-boat_size/2, boat_size/2],
                [-boat_size/2, -boat_size/2]
            ])

            rotation_matrix = np.array([
                [np.cos(state['heading']), -np.sin(state['heading'])],
                [np.sin(state['heading']), np.cos(state['heading'])]
            ])
            rotated_points = boat_points @ rotation_matrix.T
            final_points = rotated_points + np.array([state['x'], state['y']])

            boat = patches.Polygon(final_points, closed=True, color=color,
                                   edgecolor='black', linewidth=1, label=agent_id)
            self.ax.add_patch(boat)

            # ID label near the boat
            self.ax.text(state['x'], state['y'] + 10, agent_id, fontsize=8, color=color, weight='bold', ha='center')

        # Title and Legend
        plt.suptitle(f"STEP: {self.step_count}", y=0.96, fontsize=12, weight='bold')
        self.ax.legend(loc='lower right', fontsize=8)

        # Output for rgb_array
        self.fig.canvas.draw()
        image = np.asarray(self.fig.canvas.buffer_rgba())[:, :, :3]

        if self.render_mode == "human":
            plt.show(block=False)
            plt.pause(0.001)

        plt.close(self.fig)
        return image

    def close(self):
        if self.fig is not None:
            plt.close(self.fig)

"""## Train e test"""

import numpy as np
import supersuit as ss
from stable_baselines3 import PPO
from stable_baselines3.common.vec_env import VecMonitor, VecNormalize
from stable_baselines3.common.callbacks import BaseCallback

def train():
    # PettingZoo environment initialization
    env = MultiAgentSailingZoo()

    env = ss.black_death_v3(env)

    env = ss.pettingzoo_env_to_vec_env_v1(env)

    # Parallel processing and parameter sharing (only one brain for training both)
    env = ss.concat_vec_envs_v1(env, num_vec_envs=8, num_cpus=1, base_class="stable_baselines3")
    env = VecMonitor(env)
    env = VecNormalize(env, norm_obs=False, norm_reward=True, clip_reward=10.0)

    model = PPO(
        "MlpPolicy",
        env,
        learning_rate=3e-4,
        n_steps=2048,
        batch_size=128,
        n_epochs=10,
        gamma=0.99,
        gae_lambda=0.95,
        clip_range=0.2,
        ent_coef=0.01,
        verbose=1
    )

    print("Start Training (Reccomended 1kk steps)")
    model.learn(total_timesteps=1000000)

    print("Saving Model")
    model.save("ppo_sailing_marl")
    print("Model saved as 'ppo_sailing_marl.zip'")

train()

import imageio
import numpy as np
from stable_baselines3 import PPO

print("="*70)
print("ðŸŽ¬ MULTI-AGENT SAILING VIDEO")
print("="*70)

print("\n1. Loading model...")
try:
    model = PPO.load("ppo_sailing_marl")
    print("Loaded: ppo_sailing_marl")
except:
    print("No model found! Using random actions for demo.")
    model = None

for i in range(10):
    print(f"\n--- Episode {i+1} ---")

    env = MultiAgentSailingZoo(render_mode='rgb_array')
    observations, infos = env.reset()

    frames = []
    frames.append(env.render())

    step = 0

    # Loop until max_steps or agents alive
    while env.agents and step < 250:
        actions = {}

        for agent_id in env.agents:
            obs = observations[agent_id]

            if model:
                action, _ = model.predict(obs, deterministic=True)
            else:
                # Random actions if model not found
                action = env.action_space(agent_id).sample()

            actions[agent_id] = action

        observations, rewards, terminations, truncations, infos = env.step(actions)

        # Frame Creation
        frames.append(env.render())
        step += 1

    # Saving Video
    videourl = f'multi_sailing_demo_{i}.mp4'
    print(f"3. Saving video ({len(frames)} frames) to {videourl}...")
    imageio.mimsave(videourl, frames, fps=15)

    env.close()

print("\n" + "="*70)
print("âœ“ Videos created!")
print("ðŸŽ¥ Download the .mp4 files from the Colab file browser to watch.")

import glob
import ipywidgets as widgets
from IPython.display import Video, display

video_files = glob.glob("*.mp4")

video_widgets = []
for path in video_files:

    vid = Video(path, width=600, height=400, embed=True, html_attributes="autoplay muted loop playsinline")

    out = widgets.Output()
    with out:
        display(vid)
    video_widgets.append(out)

grid = widgets.GridBox(video_widgets, layout=widgets.Layout(grid_template_columns="repeat(5, 400px)"))

display(grid)

"""## Statistiche"""

try:
    model = PPO.load("ppo_sailing_marl")
except Exception as e:
    print(f"âŒ Error loading model: {e}")
    exit()

# 1. Statistics Setup
num_episodes = 1000
counts = {
    "Wins boat_0": 0,
    "Wins boat_1": 0,
    "Collisions": 0,
    "Out of Bounds": 0,
    "Timeout (250 steps)": 0
}


position_stats = {
    "boat_0": {"Inside": 0, "Outside": 0, "Win_Inside": 0, "Win_Outside": 0},
    "boat_1": {"Inside": 0, "Outside": 0, "Win_Inside": 0, "Win_Outside": 0}
}


metrics = {
  "boat_0": {"vmg": [], "triple": []},
  "boat_1": {"vmg": [], "triple": []}
}

print(f"Starting model validation on {num_episodes} episodes...")

for i in range(num_episodes):
    env = MultiAgentSailingZoo(render_mode='rgb_array')
    observations, infos = env.reset()


    current_inside_agent = None # Temporary variable for this episode

    for a in env.possible_agents:
        if env.boat_states[a]['is_inside']:
            position_stats[a]["Inside"] += 1
            current_inside_agent = a # Store who is inside
        else:
            position_stats[a]["Outside"] += 1


    terminated = False
    truncated = False
    last_infos = infos

    while not (terminated or truncated):
        actions = {}

        for agent_id in env.agents:
            obs = observations[agent_id]
            action, _states = model.predict(obs, deterministic=True)
            if isinstance(action, np.ndarray):
                action = action.item()
            actions[agent_id] = action


        observations, rewards, terminations, truncations, infos = env.step(actions)

        if infos:
            last_infos = infos

        terminated = all(terminations.values())
        truncated = all(truncations.values())


    if env.winner:
        # Global count
        if env.winner == "boat_0": counts["Wins boat_0"] += 1
        else: counts["Wins boat_1"] += 1

        if env.winner == current_inside_agent:
            # Winner started INSIDE
            position_stats[env.winner]["Win_Inside"] += 1
        else:
            # Winner started OUTSIDE
            position_stats[env.winner]["Win_Outside"] += 1

    else:
        if any(truncations.values()):
            counts["Timeout (250 steps)"] += 1
        else:
            p0 = np.array([env.boat_states["boat_0"]['x'], env.boat_states["boat_0"]['y']])
            p1 = np.array([env.boat_states["boat_1"]['x'], env.boat_states["boat_1"]['y']])
            if np.linalg.norm(p0 - p1) < (env.boat_radius * 2.1):
                counts["Collisions"] += 1
            else:
                counts["Out of Bounds"] += 1

    for a in env.possible_agents:
        agent_info = last_infos.get(a, {})
        metrics[a]["vmg"].append(agent_info.get("avg_vmg", 0))
        metrics[a]["triple"].append(env.stat_triple_turns[a])

    if (i + 1) % 10 == 0:
        print(f"Completed {i + 1} episodes...")

print("\n" + "="*80)
print(f"{'PPO VALIDATION REPORT':^80}")
print("="*80)

print(f"{'Race Outcome':<25} | {'Frequency':<10}")
print("-" * 80)
for esito, conteggio in counts.items():
    icona = "ðŸŸ¢" if "Wins" in esito else "ðŸ”´"
    print(f"{icona} {esito:<22} | {conteggio:<10}")

print("-" * 80)
print(f"{'Agent':<10} | {'Start IN':<10} | {'Wins (from IN)':<18} | {'Start OUT':<10} | {'Wins (from OUT)':<18}")
print("-" * 80)
for a in env.possible_agents:
    s_in = position_stats[a]["Inside"]
    w_in = position_stats[a]["Win_Inside"]
    s_out = position_stats[a]["Outside"]
    w_out = position_stats[a]["Win_Outside"]

    # Calculate conversion percentages (Wins / Start)
    perc_win_in = (w_in / s_in * 100) if s_in > 0 else 0.0
    perc_win_out = (w_out / s_out * 100) if s_out > 0 else 0.0

    # String formatting with percentage in parentheses
    str_w_in = f"{w_in} ({perc_win_in:.0f}%)"
    str_w_out = f"{w_out} ({perc_win_out:.0f}%)"

    print(f"{a:<10} | {s_in:<10} | {str_w_in:<18} | {s_out:<10} | {str_w_out:<18}")

print("-" * 80)
print(f"{'Agent':<10} | {'Avg VMG':<12} | {'Avg 3-Turns':<15}")
print("-" * 80)
for a in env.possible_agents:
    avg_vmg = np.mean(metrics[a]["vmg"])
    avg_triple = np.mean(metrics[a]["triple"])
    print(f"{a:<10} | {avg_vmg:<12.2f} | {avg_triple:<15.2f}")

print("="*80)
success_rate = ((counts["Wins boat_0"] + counts["Wins boat_1"]) / num_episodes) * 100
print(f" Total Success Rate: {success_rate:.1f}%")
print("="*80 + "\n")